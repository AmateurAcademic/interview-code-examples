{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import learning_curve,validation_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of this notebook\n",
    "1. I will probably use this when I have job interviews (as the interviewer) to discuss NLU engines and possibly as a reference if I am ever being interviewed for a position and NLU comes up. It can be hard for me to discuss this technical stuff very specifically that I have done working at companies due to NDAs. And honestly I can't always remember how to do all of this in detail. LOL\n",
    "2. Generally, I hope it helps developers understand how to build and implement NLU engines. I noticed this deep understanding was missing in the FOSS voice assistant community. I also looked around for a notebook or git repo for basics in NLU engines and couldn't find one (do you know of something? Please feel free to share!). Perhaps some of the methods in here can be used to produce an NLU engine that is both powerful and light enough to run inference and even be trained on low powered devices. Maybe some of my friends who are keen on Java, C++, Rust, etc. want to build a much faster engine.\n",
    "\n",
    "# NLU intent classification and entity extraction\n",
    "A natural language understanding in voice assistants focus on two problems:\n",
    "* intent classification\n",
    "    Where should the utterance (command, question, etc.) go?\n",
    "    ie the utterance 'turn off the living room lights' should be classified to 'turn off'\n",
    "* entity extraction (also known as named entity recognition, NER)\n",
    "    What are the important inputs (entities) that should be passed along\n",
    "    ie the utterance 'turn off the living room lights', the important entity is the place: 'living room'. \n",
    "\n",
    "## That's great, but don't voice assistants already have this ability? \n",
    "Yes, main stream voice assistants (Siri, Alexa, Bixby, etc.) do have this ability. However the training and inference of these general models are controlled by those companies and users can't easily add or change much, and some people have privacy concerns about running voice assistants since they don't run locally. \n",
    "\n",
    "## TinyML philosopy\n",
    "The goal of tinyML is to train and run inference of models locally by users. If a user can customize their models, then the system can 'learn' and improve based on users' preferences, instead of a 'one-size-fits-all' way of doing machine learning. \n",
    "\n",
    "## Yes, but aren't there already open source voice assistants like Mycroft and Snips/Rhasspy?\n",
    "Totally, but do you know how they work?\n",
    "\n",
    "Mycroft has two NLU engines:\n",
    "* Adapt\n",
    "* Padatious\n",
    "\n",
    "Adapt focuses on keyword word matching, RegEx patterns, and hard coding to perform these actions. Padatious uses a library called FANN (fast artifical neural network) to classify intent based on all of the words in the utterance and uses the FANN for entity edge detection. However, I haven't found that these systems perform so well (I will run a comparetitve analysis to compare results in the future). They are low powered, so that users could run them on many devices, but they aren't very powerful. \n",
    "\n",
    "Rhasspy/Snips uses two intent parsers in tandom:\n",
    "* deterministic (rule based)\n",
    "* probabilistic\n",
    "\n",
    "If the rule based approach is only applied when the first one fails to result. The rule based system uses RegEx, requiring the developer to write out these rules. The probablistic system uses linear regression for intent and conditional random fields (CRFs) for entity extraction. \n",
    "\n",
    "Wouldn't it be great to learn how to completely automate these tasks and do it with techniques light enough to run on phones or whatever? I think so. So let's do this!\n",
    "\n",
    "# Methods\n",
    "* We are going to use this data set: https://github.com/xliuhw/NLU-Evaluation-Data/blob/master/AnnotatedData/NLU-Data-Home-Domain-Annotated-All.csv\n",
    "* Detour into Word2Vec method of classifying intent (spolier alert: it doesn't work so well)\n",
    "* TFIDF encoding (this works pretty well)\n",
    "* Intent classification: A lot of classifiers to try\n",
    "    * (Gaussian) Naive Bayes Classifier\n",
    "    * Decision Tree Classifier\n",
    "    * AdaBoost Classifier\n",
    "    * K-Nearest Neighbors Classifier\n",
    "    * Random Forest Classifier\n",
    "    * Support Vector Machine Classifier\n",
    "    * maybe more in the future (XGBoost?)\n",
    "* Entity extraction: conditional random fields\n",
    "\n",
    "And finally, we bring it all together to make our NLU engine. \n",
    "\n",
    "\n",
    "# FAQ\n",
    "* Why didn't use use SPaCy, BERT (or whatever)?\n",
    "   * I wanted to choose simple stuff that could be easily found in other langauges and is low powered for inference and training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: refactoring\n",
    "* record training times and inference times for each model\n",
    "* generate report for each, save as df\n",
    "* concat all reports into df\n",
    "* return the highest scoring classifier vs training (or inference) times\n",
    "* do the same for word2vec\n",
    "* XGBoost?\n",
    "* clean up code\n",
    "* CRF feature stemmer?\n",
    "* domain classifier? (compare domain to intent classifer?)\n",
    "* make proper classes out of this to form a python NLU engine?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    data_df = pd.read_csv(file_name, sep=';')\n",
    "    return data_df.dropna(axis=0, how='any', subset=['answer_normalised'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a quick look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = load_data('NLU-Data-Home-Domain-Annotated-All.csv')\n",
    "number_of_intents = nlu_data_df['intent'].nunique()\n",
    "list_of_intents = nlu_data_df['intent'].unique()\n",
    "number_of_utterances = nlu_data_df['answer_normalised'].nunique()\n",
    "print(f'From a total of {number_of_utterances} utterances, there are {number_of_intents} intents')\n",
    "print(f'List of intents: {list_of_intents}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (skip this and the next cell if you just want TFIDF which performs better), we will keep tokenization easy \n",
    "(keep in mind, other langauges might require more complex tokenization!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(token):\n",
    "    return token.lower()\n",
    "\n",
    "def tokenize_utterances(dataframe):\n",
    "    utterances = list(dataframe.answer_normalised.values)\n",
    "    return [list(map(preprocess_lower, utterance.split(' '))) for utterance in utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_utterances = tokenize_utterances(nlu_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The target class labels (for the intents) require encoding to do machine learning stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "def encode_labels(target_class):\n",
    "    label_encoded_y = le.fit_transform(list(target_class))\n",
    "    return label_encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(label_encoded_y):\n",
    "    return le.inverse_transform(label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to predict using domains (skills), change intents to domains and use nlu_data_df.scenario.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = nlu_data_df.intent.values\n",
    "label_encoded_y = encode_labels(intents)\n",
    "label_encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_labels(label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec to create word vectors from the utterances for the classifiers\n",
    "Open question: Is this the best word embedding system in terms of performance vs resource usage?\n",
    "\n",
    "Reasons word2vec was choosen:\n",
    "* implemented in several programming langauges\n",
    "* it is well known\n",
    "* isn't too resource intensive (i.e. it could run in real time on a phone)\n",
    "\n",
    "However, it might not perform the best, bag of words methods might work better, as word order isn't super important for utterances of a voice assistant (question for the class: why?)\n",
    "\n",
    "Skip the next 4 cells if you just want the best results, go to TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_model(tokenized_utterances):\n",
    "    model = Word2Vec(tokenized_utterances, vector_size=128, window=2, min_count=1, workers=4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = create_word2vec_model(tokenized_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utterances_to_vectors(model, tokenized_utterances):\n",
    "    # get the utterances average vector\n",
    "    utterances_vectors = list()\n",
    "    for utterance in tokenized_utterances:\n",
    "        utterance_vector = [list(model.wv[token]) for token in utterance if token in model.wv.key_to_index.keys()]\n",
    "        utterances_vectors.append(list(np.mean(utterance_vector, axis=0)))\n",
    "    return utterances_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_utterances_vectors = convert_utterances_to_vectors(word2vec_model, tokenized_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "Question for class: Why does it score better?\n",
    "\n",
    "Skip this if you are checking out Word2Vec!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_utterances_vectors = vectorizer.fit_transform(nlu_data_df.answer_normalised.values)\n",
    "\n",
    "# TODO: return_train_test_split requires len for word2vec and shape for TFIDF\n",
    "# TODO: NB requires todense for TFIDF\n",
    "# TODO: Add normal bag of words and compare all of them for all models(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) In case you want to do a train_test_split (for proper evaluation, we are using cross validation otherwise)\n",
    "'What is my purpose?'\n",
    "\n",
    "'You split the data.'\n",
    "\n",
    "*sad robot noises*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_train_test_split(utterances_vectors, label_encoded_y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(utterances_vectors, label_encoded_y, train_size=0.8,test_size=0.2)\n",
    "    # if Word2Vec use len\n",
    "    # if TFIDF use shape\n",
    "    try:\n",
    "        number_of_training = x_train.shape[0]\n",
    "        number_of_testing = x_test.shape[0]\n",
    "    except:\n",
    "        number_of_training = len(x_train)\n",
    "        number_of_testing = len(x_test)\n",
    "    # (TODO: Add parameter for switching?)\n",
    "    print(f\"Training set has {number_of_training} samples.\")\n",
    "    print(f\"Testing set has {number_of_testing} samples.\")\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to specifiy whether you ar using tfidf or word2vec for your utterances_vectors!\n",
    "x_train, x_test, y_train, y_test = return_train_test_split(utterances_vectors, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ohhhh, machine learning!\n",
    "\n",
    "The classifiers are chosen because:\n",
    "* Most of these algorithmns exist in other langauges\n",
    "* They are pretty light (ie can run on a phone not just for inference but for TRAINING custom models!)\n",
    "* Word order doesn't matter (bag of words style over here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, x_train, y_train):\n",
    "    # TODO: add in training time\n",
    "    return classifier.fit(x_train, y_train)\n",
    "\n",
    "def test_classifier(classifier_model, x_test, y_test):\n",
    "    y_prediction = classifier_model.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_prediction, average='micro')\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea if these settings are good or not, might want to do some grid search based tuning or something.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add more like LinearRegression\n",
    "DT = DecisionTreeClassifier(random_state=42)\n",
    "ADA = AdaBoostClassifier(n_estimators=100)\n",
    "KN = KNeighborsClassifier(n_neighbors=100)\n",
    "RF = RandomForestClassifier()\n",
    "SVM = svm.SVC(gamma='scale')\n",
    "NB = GaussianNB()\n",
    "\n",
    "classifiers = [DT, ADA, KN, RF, SVM, NB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate all classifiers\n",
    "Warning this could take a long time, it should only be used to reproduce the reports\n",
    "\n",
    "Unless you do want to reproduce the reports, skip the next two cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_classifier(classifier, x_train, y_train):\n",
    "    start = time.time()\n",
    "    print(f'Cross validating with {str(classifier)}')\n",
    "    if x_train is tfidf_utterances_vectors and classifier is NB:\n",
    "        # note: I threw NB at the end so it doesn't set them all to dense\n",
    "            x_train = x_train.todense()\n",
    "    prediction = cross_val_predict(estimator=classifier, X=x_train, y=y_train, cv=5)\n",
    "    stop = time.time()\n",
    "    duration = stop - start\n",
    "    print(f'Time it took to cross validate {str(classifier)}: {duration}')\n",
    "    return prediction\n",
    "\n",
    "def generate_report(classifier, prediction, y_train):\n",
    "    prediction_decoded = decode_labels(prediction).tolist()\n",
    "    y_train_decoded = decode_labels(y_train).tolist()\n",
    "    report = classification_report(y_pred=prediction_decoded, y_true=y_train_decoded, output_dict=True)\n",
    "    print(f'Generating report for {classifier}')\n",
    "    return report\n",
    "\n",
    "def convert_report_to_df(classifier, report):\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df['classifier'] = str(classifier)\n",
    "    df.index = df.index.set_names(['intent'])\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def evaluate_classifier(classifier, x_train, y_train):\n",
    "    prediction = cross_validate_classifier(classifier, x_train, y_train)\n",
    "    report = generate_report(classifier, prediction, y_train)\n",
    "    return convert_report_to_df(classifier, report)\n",
    "\n",
    "def evaluate_all_classifiers(classifiers, x_train, y_train):\n",
    "    for count, classifier in enumerate(classifiers):\n",
    "        df = evaluate_classifier(classifier, x_train, y_train)\n",
    "        if count is 0:\n",
    "            concat_df = df\n",
    "        else:\n",
    "            concat_df = pd.concat([concat_df, df])\n",
    "    return concat_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classififiers_tfidf_df = evaluate_all_classifiers(classifiers, tfidf_utterances_vectors, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load up our report and take a look\n",
    "\n",
    "It looks like SVM scores slightly higher than RF, but the trade-off for performance is worth it with RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classififiers_tfidf_df = pd.read_csv('analysis_of_all_intent_classifiers_with_tfidf.csv')\n",
    "report_all_intent_classififiers_tfidf_df[report_all_intent_classififiers_tfidf_df['intent'].str.contains('accuracy')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at our RF, we can see that some intents score pretty poorly: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classififiers_tfidf_df[report_all_intent_classififiers_tfidf_df['classifier'].str.contains(str(RF))].sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'dislikeness' scores pretty poorly with every classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classififiers_tfidf_df[report_all_intent_classififiers_tfidf_df['intent'].str.contains('dislikeness')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for 'quirky'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classififiers_tfidf_df[report_all_intent_classififiers_tfidf_df['intent'].str.contains('quirky')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'volume_other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classififiers_tfidf_df[report_all_intent_classififiers_tfidf_df['intent'].str.contains('volume_other')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'settings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classififiers_tfidf_df[report_all_intent_classififiers_tfidf_df['intent'].str.contains('settings')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all models: haven't tested ALL word2vec yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "* It is clear to see that some of the utterances suck and some of the intents are overlapping\n",
    "* I think on a real data set you could see ~10% improved performance\n",
    "* In addition\n",
    "    * if one were to use 'input typing' (entity typing) with the entity tagger\n",
    "    * then if the types of entities the entity tagger returns are not correct types, this could be used to improve classification\n",
    "    * Also if required entities are missing, a reply could be given saying a certain entity is required (ie utterance: 'set a timer', response: 'how long of a timer' because expected required entity is duration)\n",
    "* There are a lot of rules that can be addeded between the two classifiers (intent and entity tagging), that could boost the model, in addition to fine tuning the model itself.\n",
    "\n",
    "It is my opinion that a random forest with TFIDF and a CRF entity tagger would work fine for NLU tasks, even on under-powered devices, including TRAINING! Here's hoping someone comes along and bangs out a production level NLU engine in a super fast langauge! ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winner is RF with TFIDF!\n",
    "Unless pure performance is your goal, then SVM for the win. But it is nice to balance out performance vs speed (training and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = train_classifier(RF, tfidf_utterances_vectors, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the intent label from an utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(classifier_model, utterance):\n",
    "    utterance = utterance.lower()\n",
    "    transformed_utterance = vectorizer.transform([utterance])\n",
    "    predicted_label = classifier_model.predict(transformed_utterance)\n",
    "    return decode_labels(predicted_label)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out yourself with an utterance\n",
    "utterance = 'Turn the livingroom lights off'\n",
    "label = predict_label(RF_model, utterance)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it get wrong and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrectly_classified_utterances(classifier_model, utterances, tfidf_utterances_vectors, label_encoded_y):\n",
    "    y_prediction = classifier_model.predict(tfidf_utterances_vectors)\n",
    "    for utterance, prediction, intent in zip(utterances, decode_labels(y_prediction), decode_labels(label_encoded_y)):\n",
    "        if str(prediction) not in str(intent):\n",
    "            print(f'{utterance} has been classified as {prediction}, but it should be {intent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df['answer_normalised'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df[(nlu_data_df['answer_normalised'].str.fullmatch('a')) & (nlu_data_df['intent'].str.contains('factoid'))]\n",
    "\n",
    "#remove: answer id: 19126.0, 21940.0, 21942.0, 25765.0, 4274.0\n",
    "# go by user ID too? ie 981.0, 107.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at these its important to know\n",
    "# It is easy to see there is a lot of garbage in this data set that hurts performance\n",
    "get_incorrectly_classified_utterances(RF_model, nlu_data_df['answer_normalised'].tolist(), tfidf_utterances_vectors, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the entities from the utterances with their taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seperate_types_and_entities(entities):\n",
    "    entity_list = []\n",
    "    for entity in entities:\n",
    "        split_entity = entity.split(' : ')\n",
    "        entity_type = split_entity[0]\n",
    "        entity_text = split_entity[1].split(' ')\n",
    "        entity_list.append({'type':entity_type, 'words': entity_text})\n",
    "    return entity_list\n",
    "\n",
    "def extract_entities(utterance):\n",
    "    # match [...]: \\[[^][]*]\n",
    "    entities = re.findall(r'\\[(.*?)\\]', utterance)\n",
    "    return seperate_types_and_entities(entities)\n",
    "    # extract label and entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_with_tagging = 'wake me up at [time : five pm] [date : this week]'\n",
    "\n",
    "entities = extract_entities(utterance_with_tagging)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging and entity labeling of utterances\n",
    "Conditional random fields just love features. One of the most obvious features we could give it besides the words themselves are the part of speech (POS) tags of the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_utterance(utterance):\n",
    "    tokenized_utterance = nltk.word_tokenize(utterance)\n",
    "    utterance_pos = nltk.pos_tag(tokenized_utterance)\n",
    "    return utterance_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = 'wake me up at five pm this week'\n",
    "utterance_pos = pos_tag_utterance(utterance)\n",
    "utterance_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pos_and_entity_tags(entities, utterance_pos):\n",
    "    output = []\n",
    "    words = []\n",
    "\n",
    "    for entity in entities:\n",
    "        for word in entity['words']:\n",
    "            words.append(word)\n",
    "\n",
    "    for pair in utterance_pos:\n",
    "        word = pair[0]\n",
    "        pos = pair[1]\n",
    "        for entity in entities:\n",
    "            if word in entity['words']:\n",
    "                entity_type = entity['type']\n",
    "                output.append((word, pos, entity_type))\n",
    "            elif word not in words and entity is entities[-1]:\n",
    "                entity_type = '0'\n",
    "                output.append((word, pos, entity_type))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_pos_and_entity_tags(entities, utterance_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's put it all together to rip the features out in the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_feature_dataset(nlu_data_df):\n",
    "    feature_dataset = []\n",
    "    for utterance, utterance_with_tagging in zip(nlu_data_df['answer_normalised'], nlu_data_df['answer_annotation']):\n",
    "        print(utterance)\n",
    "        entities = extract_entities(utterance_with_tagging)\n",
    "        utterance_pos = pos_tag_utterance(utterance)\n",
    "        feature_dataset.append(combine_pos_and_entity_tags(entities, utterance_pos))\n",
    "    return feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = create_feature_dataset(nlu_data_df)\n",
    "feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor code\n",
    "# try removing the word-level slicing (I don't think prefixes and suffixes give more info in English)\n",
    "# try add stemming (lemma?) or something as an extra feature?\n",
    "def word2features(utterance, i):\n",
    "    word = utterance[i][0]\n",
    "    postag = utterance[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = utterance[i-1][0]\n",
    "        postag1 = utterance[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word': word1,\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(utterance)-1:\n",
    "        word1 = utterance[i+1][0]\n",
    "        postag1 = utterance[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word': word1,\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def utterance2features(utterance):\n",
    "    return [word2features(utterance, i) for i in range(len(utterance))]\n",
    "\n",
    "def utterance2labels(utterance):\n",
    "    return [label for token, postag, label in utterance]\n",
    "\n",
    "def utterance2tokens(utterance):\n",
    "    return [token for token, postag, label in utterance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [utterance2features(utterance) for utterance in feature_dataset]\n",
    "y = [utterance2labels(utterance) for utterance in feature_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: it is easy to see that for entities with few examples, the results are very poor.\n",
    "\n",
    "Unless you want to reproduce the report, you can skip this next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "# TODO: Use this for every classifier for intent also\n",
    "report = flat_classification_report(y_pred=pred, y_true=y, output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.index = df.index.set_names(['entity-type'])\n",
    "df = df.reset_index()\n",
    "df.to_csv('analysis_of_CRF_for_entity_extraction.csv', index=False)\n",
    "\n",
    "# TODO: Load as CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the entities with the fewest examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get counts for each entity type, then drop these:\n",
    "remove_strings = ['audiobook_author', 'audiobook_name', 'cooking_type', 'drink_type', 'email_address', 'email_folder', 'game_name', 'game_type', 'ingredient', 'movie_name', 'movie_type', 'music_album', 'music_descriptor', 'news_topic', 'personal_info', 'podcast_descriptor', 'podcast_name', 'query_detail', 'radio_name', 'song_name', 'sport_type', 'transport_descriptor', 'transport_name', 'transport_type']\n",
    "\n",
    "nlu_data_entities_cleaned_df = nlu_data_df[~nlu_data_df['answer_annotation'].str.contains('|'.join(remove_strings))]\n",
    "nlu_data_entities_cleaned_df\n",
    "# TODO: fix the one entity type label with no space after entity type (ie type: thing -> type : thing)\n",
    "# TODO: change nlu_data_df to cleaned for rest of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = create_feature_dataset(nlu_data_entities_cleaned_df)\n",
    "X = [utterance2features(utterance) for utterance in feature_dataset]\n",
    "y = [utterance2labels(utterance) for utterance in feature_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we want to use this as an entity extraction engine, we will need to get the entities, their types, and the location of the entities in the utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(utterance):\n",
    "    utterance_pos = pos_tag_utterance(utterance)\n",
    "    utterance_features = utterance2features(utterance_pos)\n",
    "    label = crf_model.predict_single(utterance_features)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_types_and_locations(utterance):\n",
    "    entity_locations_and_types = []\n",
    "    entities = get_entities(utterance)\n",
    "    for location, entity in enumerate(entities):\n",
    "        if entity is not '0':\n",
    "            entity_locations_and_types.append((location, entity))\n",
    "    return entity_locations_and_types\n",
    "\n",
    "def get_entity_tags(utterance):\n",
    "    entity_locations_and_types = get_entity_types_and_locations(utterance)\n",
    "    split_utterance = utterance.split(' ')\n",
    "    tagged_entities = [(entity_type, split_utterance[location]) for location, entity_type in entity_locations_and_types]\n",
    "    return tagged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = 'set an alarm for five pm'\n",
    "get_entity_tags(utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's bring it all together, a full NLU engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Maybe give this function a better name?\n",
    "def get_NLU_stuff(utterance):\n",
    "    tagged_entities  = get_entity_tags(utterance)\n",
    "    return [utterance, predict_label(RF_model, utterance), tagged_entities]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random test utterances I could come up with, maybe add some of your own and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [\n",
    "    'vacuum the bathroom',\n",
    "    'clean the hall',\n",
    "    'what is the weather like this weekend',\n",
    "     'what is the weather like in munich tomorrow',\n",
    "     'what is the temperature',\n",
    "     'will it rain today',\n",
    "     'turn off the kitchen lights',\n",
    "     'turn on the living room lights',\n",
    "     'set an alarm for five pm',\n",
    "     'set an alarm for ten am',\n",
    "     'what time is it in new york',\n",
    "     'what time is it in berlin in two hours from now',\n",
    "     'tell me a joke',\n",
    "     'how are you',\n",
    "     'when was biden born',\n",
    "     'how long does it take to boil an egg',\n",
    "     'how do you make a caesar salad',\n",
    "     'how much is a euro in dollars'\n",
    "]\n",
    "\n",
    "for utterance in utterances:\n",
    "    print(get_NLU_stuff(utterance))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a546bd87e52c3aef9807d84fb5760e9bbca867abb60a3fd598826e0271da7992"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
