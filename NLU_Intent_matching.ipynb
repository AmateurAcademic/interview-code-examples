{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of this notebook\n",
    "1. I will probably use this when I have job interviews (as the interviewer) to discuss NLU engines and possibly as a reference if I am ever being interviewed for a position and NLU comes up. It can be hard for me to discuss this technical stuff very specifically that I have done working at companies due to NDAs. And honestly I can't always remember how to do all of this in detail. LOL\n",
    "2. Generally, I hope it helps developers understand how to build and implement NLU engines. I noticed this deep understanding was missing in the FOSS voice assistant community. I also looked around for a notebook or git repo for basics in NLU engines and couldn't find one (do you know of something? Please feel free to share!). Perhaps some of the methods in here can be used to produce an NLU engine that is both powerful and light enough to run inference and even be trained on low powered devices. Maybe some of my friends who are keen on Java, C++, Rust, etc. want to build a much faster engine.\n",
    "3. It is always good to have a notebook for benchmarking methods.\n",
    "\n",
    "# NLU intent classification and entity extraction\n",
    "Natural language understanding in voice assistants focuses on two problems:\n",
    "* intent classification\n",
    "    Where should the utterance (command, question, etc.) go?\n",
    "    ie the utterance 'turn off the living room lights' should be classified to the intent 'turn off'\n",
    "* entity extraction (also known as named entity recognition, NER)\n",
    "    What are the important inputs (entities) that should be passed along\n",
    "    ie the utterance 'turn off the living room lights', the important entity is the place: 'living room'. \n",
    "\n",
    "## TinyML philosopy\n",
    "The goal of tinyML is to train and run inference of models locally by users. If a user can customize their models, then the system can 'learn' and improve based on users' preferences, instead of a 'one-size-fits-all' way of doing machine learning. \n",
    "\n",
    "## Yes, but aren't there already open source voice assistants like Mycroft and Snips/Rhasspy?\n",
    "Mycroft has two NLU engines:\n",
    "* Adapt\n",
    "* Padatious\n",
    "\n",
    "Adapt focuses on keyword word matching, RegEx patterns, and hard coding to perform these actions. Padatious uses a library called FANN (fast artifical neural network) to classify intent based on all of the words in the utterance and uses the FANN for entity edge detection. They are low powered, so that users could run them on many devices, but they aren't very powerful. \n",
    "\n",
    "Rhasspy/Snips uses two intent parsers in tandom:\n",
    "* deterministic (rule based)\n",
    "* probabilistic\n",
    "\n",
    "The rule based approach is only applied when the first one fails to result. The rule based system uses RegEx, requiring the developer to write out these rules. The probablistic system uses logistic regression for intent and conditional random fields (CRFs) for entity extraction. \n",
    "\n",
    "Wouldn't it be great to learn how to completely automate these tasks and do it with techniques light enough to run on phones or whatever? I think so. So let's do this!\n",
    "\n",
    "# Methods\n",
    "* We are going to use this data set: https://github.com/xliuhw/NLU-Evaluation-Data/blob/master/AnnotatedData/NLU-Data-Home-Domain-Annotated-All.csv\n",
    "* Detour into Word2Vec method of classifying intent (spolier alert: it doesn't work so well)\n",
    "* TFIDF encoding (this works pretty well)\n",
    "* Intent classification: A lot of classifiers to try\n",
    "    * Logistic Regression\n",
    "    * Decision Tree Classifier\n",
    "    * AdaBoost Classifier\n",
    "    * K-Nearest Neighbors Classifier\n",
    "    * Random Forest Classifier\n",
    "    * Support Vector Machine Classifier\n",
    "    * (Gaussian) Naive Bayes Classifier\n",
    "* Entity extraction: conditional random fields\n",
    "\n",
    "And finally, we bring it all together to make our prototype NLU engine. \n",
    "\n",
    "\n",
    "# FAQ\n",
    "* Why didn't use use SPaCy, BERT (or whatever)?\n",
    "   * I wanted to choose simple stuff that could be easily found in other langauges and is low powered for inference and training, also I tried to write the code as simple as possible, so it would be easy to understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* record training times and inference times for each model (on raspi4)\n",
    "* CRF feature stemmer?\n",
    "* domain classifier? (compare domain to intent classifer?)\n",
    "* make proper classes out of this to form a generic python NLU engine?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    data_df = pd.read_csv(file_name, sep=';')\n",
    "    return data_df.dropna(axis=0, how='any', subset=['answer_normalised'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a quick look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = load_data('NLU-Data-Home-Domain-Annotated-All.csv')\n",
    "number_of_intents = nlu_data_df['intent'].nunique()\n",
    "list_of_intents = nlu_data_df['intent'].unique()\n",
    "number_of_utterances = nlu_data_df['answer_normalised'].nunique()\n",
    "print(f'From a total of {number_of_utterances} utterances, there are {number_of_intents} intents')\n",
    "print(f'List of intents: {list_of_intents}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utterances only have one word or even one letter! We should remove those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = nlu_data_df[nlu_data_df['answer_normalised'].str.contains(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (skip this and the next cell if you just want TFIDF which performs better), we will keep tokenization easy \n",
    "(keep in mind, other langauges might require more complex tokenization!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(token):\n",
    "    return token.lower()\n",
    "\n",
    "def tokenize_utterances(dataframe):\n",
    "    utterances = list(dataframe.answer_normalised.values)\n",
    "    return [list(map(preprocess_lower, utterance.split(' '))) for utterance in utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_utterances = tokenize_utterances(nlu_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = nlu_data_df['answer_normalised']\n",
    "## create list of lists of unigrams\n",
    "list_utterances = []\n",
    "for utterance in utterances:\n",
    "   list_words = utterance.split()\n",
    "   list_grams = [\" \".join(list_words[i:i+1]) \n",
    "               for i in range(0, len(list_words), 1)]\n",
    "   list_utterances.append(list_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The target class labels (for the intents) require encoding to do machine learning stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "def encode_labels(target_class):\n",
    "    label_encoded_y = le.fit_transform(target_class)\n",
    "    return label_encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(label_encoded_y):\n",
    "    return le.inverse_transform(label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to predict using domains (skills), change intents to domains and use nlu_data_df.scenario.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = nlu_data_df.intent.values\n",
    "label_encoded_y = encode_labels(intents)\n",
    "label_encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_labels(label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec to create word vectors from the utterances for the classifiers\n",
    "Open question: Is this the best word embedding system in terms of performance vs resource usage?\n",
    "\n",
    "Reasons word2vec was choosen:\n",
    "* implemented in several programming langauges\n",
    "* it is well known\n",
    "* isn't too resource intensive (i.e. it could run in real time on a phone)\n",
    "\n",
    "However, it might not perform the best, bag of words methods might work better, as word order isn't super important for utterances of a voice assistant (question for the class: why?)\n",
    "\n",
    "Skip the next 4 cells if you just want the best results, go to TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_model(tokenized_utterances):\n",
    "    model = Word2Vec(tokenized_utterances, vector_size=128, window=2, min_count=1, workers=4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = create_word2vec_model(tokenized_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utterances_to_vectors(model, tokenized_utterances):\n",
    "    # get the utterances average vector\n",
    "    utterances_vectors = list()\n",
    "    for utterance in tokenized_utterances:\n",
    "        utterance_vector = [list(model.wv[token]) for token in utterance if token in model.wv.key_to_index.keys()]\n",
    "        utterances_vectors.append(list(np.mean(utterance_vector, axis=0)))\n",
    "    return utterances_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_utterances_vectors = convert_utterances_to_vectors(word2vec_model, tokenized_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "Question for class: Why does it score better?\n",
    "\n",
    "Skip this if you are checking out Word2Vec!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_utterances_vectors = vectorizer.fit_transform(nlu_data_df.answer_normalised.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ohhhh, machine learning!\n",
    "\n",
    "The classifiers are chosen because:\n",
    "* Most of these algorithmns exist in other langauges\n",
    "* They are pretty light (ie can run on a phone not just for inference but for TRAINING custom models!)\n",
    "* Word order doesn't matter (bag of words style over here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, x_train, y_train):\n",
    "    # TODO: add in training time\n",
    "    return classifier.fit(x_train, y_train)\n",
    "\n",
    "def test_classifier(classifier_model, x_test, y_test):\n",
    "    y_prediction = classifier_model.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_prediction, average='micro')\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea if these settings are good or not, might want to do some grid search based tuning or something.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(solver='liblinear', random_state=0)\n",
    "DT = DecisionTreeClassifier(random_state=42)\n",
    "ADA = AdaBoostClassifier(n_estimators=100)\n",
    "KN = KNeighborsClassifier(n_neighbors=100)\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\n",
    "SVM = svm.SVC(gamma='scale')\n",
    "NB = GaussianNB()\n",
    "\n",
    "classifiers = [LR, DT, ADA, KN, RF, SVM, NB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate all classifiers\n",
    "Warning this could take a long time, it should only be used to reproduce the reports\n",
    "\n",
    "Unless you do want to reproduce the reports, skip the next 4 cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_classifier(classifier, x_train, y_train):\n",
    "    start = time.time()\n",
    "    print(f'Cross validating with {str(classifier)}')\n",
    "    try:\n",
    "        if x_train is tfidf_utterances_vectors and classifier is NB:\n",
    "            # note: I threw NB at the end so it doesn't set them all to dense\n",
    "                x_train = x_train.todense()\n",
    "    except:\n",
    "        pass\n",
    "    prediction = cross_val_predict(estimator=classifier, X=x_train, y=y_train, cv=5)\n",
    "    stop = time.time()\n",
    "    duration = stop - start\n",
    "    print(f'Time it took to cross validate {str(classifier)}: {duration}')\n",
    "    return prediction\n",
    "\n",
    "def generate_report(classifier, prediction, y_train):\n",
    "    prediction_decoded = decode_labels(prediction).tolist()\n",
    "    y_train_decoded = decode_labels(y_train).tolist()\n",
    "    report = classification_report(y_pred=prediction_decoded, y_true=y_train_decoded, output_dict=True)\n",
    "    print(f'Generating report for {classifier}')\n",
    "    return report\n",
    "\n",
    "def convert_report_to_df(classifier, report):\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df['classifier'] = str(classifier)\n",
    "    df.index = df.index.set_names(['intent'])\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def evaluate_classifier(classifier, x_train, y_train):\n",
    "    prediction = cross_validate_classifier(classifier, x_train, y_train)\n",
    "    report = generate_report(classifier, prediction, y_train)\n",
    "    return convert_report_to_df(classifier, report)\n",
    "\n",
    "def evaluate_all_classifiers(classifiers, x_train, y_train):\n",
    "    for count, classifier in enumerate(classifiers):\n",
    "        df = evaluate_classifier(classifier, x_train, y_train)\n",
    "        if count is 0:\n",
    "            concat_df = df\n",
    "        else:\n",
    "            concat_df = pd.concat([concat_df, df])\n",
    "    return concat_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classifiers_tfidf_df = evaluate_all_classifiers(classifiers, tfidf_utterances_vectors, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the encoding type\n",
    "`tfidf`, `word2vec`, or something else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classifiers_tfidf_df['encoding'] = 'tfidf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the dataframe and seperate overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classifiers_tfidf_df['classifier'] = report_all_intent_classifiers_tfidf_df['classifier'].str.replace(r\"\\([^()]*\\)\", \"\")\n",
    "report_all_intent_classifiers_overview_df = report_all_intent_classifiers_tfidf_df[report_all_intent_classifiers_tfidf_df['intent'].str.contains('accuracy|avg')]\n",
    "report_all_intent_classifiers_overview_df = report_all_intent_classifiers_overview_df.rename(columns={'intent': 'measure'})\n",
    "report_all_intent_classifiers_overview_df = report_all_intent_classifiers_overview_df.to_csv('report_all_intent_classifiers_overview.csv', index=False)\n",
    "\n",
    "\n",
    "report_all_intent_classifiers_tfidf_df = report_all_intent_classifiers_tfidf_df[~report_all_intent_classifiers_tfidf_df['intent'].str.contains('accuracy|avg')]\n",
    "report_all_intent_classifiers_tfidf_df.to_csv('report_all_intent_classifiers_tfidf.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load up our reports and take a look\n",
    "\n",
    "It looks like SVM scores slightly higher than LR, but the trade-off for performance is worth it with LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.763347</td>\n",
       "      <td>0.763347</td>\n",
       "      <td>0.763347</td>\n",
       "      <td>0.763347</td>\n",
       "      <td>SVC</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.759560</td>\n",
       "      <td>0.759560</td>\n",
       "      <td>0.759560</td>\n",
       "      <td>0.759560</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.754481</td>\n",
       "      <td>0.754481</td>\n",
       "      <td>0.754481</td>\n",
       "      <td>0.754481</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.751290</td>\n",
       "      <td>0.751290</td>\n",
       "      <td>0.751290</td>\n",
       "      <td>0.751290</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.669834</td>\n",
       "      <td>0.669834</td>\n",
       "      <td>0.669834</td>\n",
       "      <td>0.669834</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.663709</td>\n",
       "      <td>0.663709</td>\n",
       "      <td>0.663709</td>\n",
       "      <td>0.663709</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.436701</td>\n",
       "      <td>0.436701</td>\n",
       "      <td>0.436701</td>\n",
       "      <td>0.436701</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.408585</td>\n",
       "      <td>0.408585</td>\n",
       "      <td>0.408585</td>\n",
       "      <td>0.408585</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     measure  precision    recall  f1-score   support              classifier  \\\n",
       "15  accuracy   0.763347  0.763347  0.763347  0.763347                     SVC   \n",
       "18  accuracy   0.759560  0.759560  0.759560  0.759560      LogisticRegression   \n",
       "21  accuracy   0.754481  0.754481  0.754481  0.754481           XGBClassifier   \n",
       "3   accuracy   0.751290  0.751290  0.751290  0.751290  RandomForestClassifier   \n",
       "12  accuracy   0.669834  0.669834  0.669834  0.669834    KNeighborsClassifier   \n",
       "6   accuracy   0.663709  0.663709  0.663709  0.663709  DecisionTreeClassifier   \n",
       "0   accuracy   0.436701  0.436701  0.436701  0.436701              GaussianNB   \n",
       "9   accuracy   0.408585  0.408585  0.408585  0.408585      AdaBoostClassifier   \n",
       "\n",
       "   encoding  \n",
       "15    tfidf  \n",
       "18    tfidf  \n",
       "21    tfidf  \n",
       "3     tfidf  \n",
       "12    tfidf  \n",
       "6     tfidf  \n",
       "0     tfidf  \n",
       "9     tfidf  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_all_intent_classifiers_overview_df = pd.read_csv('report_all_intent_classifiers_overview.csv')\n",
    "report_all_intent_classifiers_overview_df[report_all_intent_classifiers_overview_df['measure'].str.contains('accuracy')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_all_intent_classifiers_tfidf_df = pd.read_csv('report_all_intent_classifiers_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at our LR, we can see that some intents score pretty poorly (the ones with low support): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>greet</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>convert</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>97.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.425422</td>\n",
       "      <td>0.327345</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>hue_lighton</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>80.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>likeness</td>\n",
       "      <td>0.819149</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>200.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>querycontact</td>\n",
       "      <td>0.809917</td>\n",
       "      <td>0.447489</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>219.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>events</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.468553</td>\n",
       "      <td>0.599598</td>\n",
       "      <td>318.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.471910</td>\n",
       "      <td>0.613139</td>\n",
       "      <td>89.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>movies</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>109.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>factoid</td>\n",
       "      <td>0.689011</td>\n",
       "      <td>0.601727</td>\n",
       "      <td>0.642418</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>locations</td>\n",
       "      <td>0.841772</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.642512</td>\n",
       "      <td>256.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>game</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>232.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>maths</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.554140</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>157.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>volume_mute</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.561290</td>\n",
       "      <td>0.710204</td>\n",
       "      <td>155.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>order</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>198.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>volume_down</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>73.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>audiobook</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.616034</td>\n",
       "      <td>0.743003</td>\n",
       "      <td>237.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>query</td>\n",
       "      <td>0.653977</td>\n",
       "      <td>0.879620</td>\n",
       "      <td>0.750199</td>\n",
       "      <td>5898.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>createoradd</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.757679</td>\n",
       "      <td>0.752542</td>\n",
       "      <td>293.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>volume_up</td>\n",
       "      <td>0.786260</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.765799</td>\n",
       "      <td>138.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>recipe</td>\n",
       "      <td>0.908784</td>\n",
       "      <td>0.662562</td>\n",
       "      <td>0.766382</td>\n",
       "      <td>406.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>sendemail</td>\n",
       "      <td>0.696827</td>\n",
       "      <td>0.865693</td>\n",
       "      <td>0.772135</td>\n",
       "      <td>685.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>definition</td>\n",
       "      <td>0.916898</td>\n",
       "      <td>0.675510</td>\n",
       "      <td>0.777908</td>\n",
       "      <td>490.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>80.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>ticket</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>0.802603</td>\n",
       "      <td>239.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>wemo_off</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.744898</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>98.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>music</td>\n",
       "      <td>0.756011</td>\n",
       "      <td>0.904399</td>\n",
       "      <td>0.823575</td>\n",
       "      <td>1182.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>radio</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.773481</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>543.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>hue_lightchange</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.807175</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>223.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>hue_lightup</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>141.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>stock</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.736059</td>\n",
       "      <td>0.842553</td>\n",
       "      <td>269.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>post</td>\n",
       "      <td>0.938356</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.843077</td>\n",
       "      <td>537.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>set</td>\n",
       "      <td>0.844318</td>\n",
       "      <td>0.863953</td>\n",
       "      <td>0.854023</td>\n",
       "      <td>1720.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>hue_lightoff</td>\n",
       "      <td>0.835249</td>\n",
       "      <td>0.889796</td>\n",
       "      <td>0.861660</td>\n",
       "      <td>245.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>hue_lightdim</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.863436</td>\n",
       "      <td>125.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>joke</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.776860</td>\n",
       "      <td>0.866359</td>\n",
       "      <td>121.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>traffic</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>194.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>cleaning</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>172.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>podcasts</td>\n",
       "      <td>0.986971</td>\n",
       "      <td>0.816712</td>\n",
       "      <td>0.893805</td>\n",
       "      <td>371.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>remove</td>\n",
       "      <td>0.980930</td>\n",
       "      <td>0.844103</td>\n",
       "      <td>0.907387</td>\n",
       "      <td>975.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>taxi</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.841530</td>\n",
       "      <td>0.911243</td>\n",
       "      <td>183.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>coffee</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>197.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>currency</td>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.930612</td>\n",
       "      <td>378.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              intent  precision    recall  f1-score  support  \\\n",
       "318     volume_other   0.000000  0.000000  0.000000     24.0   \n",
       "284      dislikeness   0.000000  0.000000  0.000000     25.0   \n",
       "288            greet   0.000000  0.000000  0.000000     21.0   \n",
       "280          convert   1.000000  0.175258  0.298246     97.0   \n",
       "305           quirky   0.425422  0.327345  0.369994   1002.0   \n",
       "292      hue_lighton   0.916667  0.289474  0.440000     38.0   \n",
       "311         settings   0.866667  0.325000  0.472727     80.0   \n",
       "295         likeness   0.819149  0.385000  0.523810    200.0   \n",
       "304     querycontact   0.809917  0.447489  0.576471    219.0   \n",
       "285           events   0.832402  0.468553  0.599598    318.0   \n",
       "276       addcontact   0.875000  0.471910  0.613139     89.0   \n",
       "298           movies   0.841270  0.486239  0.616279    109.0   \n",
       "286          factoid   0.689011  0.601727  0.642418   1042.0   \n",
       "296        locations   0.841772  0.519531  0.642512    256.0   \n",
       "287             game   0.952381  0.517241  0.670391    232.0   \n",
       "297            maths   0.935484  0.554140  0.696000    157.0   \n",
       "317      volume_mute   0.966667  0.561290  0.710204    155.0   \n",
       "300            order   0.853147  0.616162  0.715543    198.0   \n",
       "316      volume_down   0.916667  0.602740  0.727273     73.0   \n",
       "277        audiobook   0.935897  0.616034  0.743003    237.0   \n",
       "303            query   0.653977  0.879620  0.750199   5898.0   \n",
       "281      createoradd   0.747475  0.757679  0.752542    293.0   \n",
       "319        volume_up   0.786260  0.746377  0.765799    138.0   \n",
       "307           recipe   0.908784  0.662562  0.766382    406.0   \n",
       "309        sendemail   0.696827  0.865693  0.772135    685.0   \n",
       "283       definition   0.916898  0.675510  0.777908    490.0   \n",
       "321          wemo_on   0.981481  0.662500  0.791045     80.0   \n",
       "314           ticket   0.833333  0.774059  0.802603    239.0   \n",
       "320         wemo_off   0.912500  0.744898  0.820225     98.0   \n",
       "299            music   0.756011  0.904399  0.823575   1182.0   \n",
       "306            radio   0.909091  0.773481  0.835821    543.0   \n",
       "289  hue_lightchange   0.873786  0.807175  0.839161    223.0   \n",
       "293      hue_lightup   0.902439  0.787234  0.840909    141.0   \n",
       "312            stock   0.985075  0.736059  0.842553    269.0   \n",
       "302             post   0.938356  0.765363  0.843077    537.0   \n",
       "310              set   0.844318  0.863953  0.854023   1720.0   \n",
       "291     hue_lightoff   0.835249  0.889796  0.861660    245.0   \n",
       "290     hue_lightdim   0.960784  0.784000  0.863436    125.0   \n",
       "294             joke   0.979167  0.776860  0.866359    121.0   \n",
       "315          traffic   0.968553  0.793814  0.872521    194.0   \n",
       "278         cleaning   0.985714  0.802326  0.884615    172.0   \n",
       "301         podcasts   0.986971  0.816712  0.893805    371.0   \n",
       "308           remove   0.980930  0.844103  0.907387    975.0   \n",
       "313             taxi   0.993548  0.841530  0.911243    183.0   \n",
       "279           coffee   0.988304  0.857868  0.918478    197.0   \n",
       "282         currency   0.957983  0.904762  0.930612    378.0   \n",
       "\n",
       "             classifier encoding  \n",
       "318  LogisticRegression    tfidf  \n",
       "284  LogisticRegression    tfidf  \n",
       "288  LogisticRegression    tfidf  \n",
       "280  LogisticRegression    tfidf  \n",
       "305  LogisticRegression    tfidf  \n",
       "292  LogisticRegression    tfidf  \n",
       "311  LogisticRegression    tfidf  \n",
       "295  LogisticRegression    tfidf  \n",
       "304  LogisticRegression    tfidf  \n",
       "285  LogisticRegression    tfidf  \n",
       "276  LogisticRegression    tfidf  \n",
       "298  LogisticRegression    tfidf  \n",
       "286  LogisticRegression    tfidf  \n",
       "296  LogisticRegression    tfidf  \n",
       "287  LogisticRegression    tfidf  \n",
       "297  LogisticRegression    tfidf  \n",
       "317  LogisticRegression    tfidf  \n",
       "300  LogisticRegression    tfidf  \n",
       "316  LogisticRegression    tfidf  \n",
       "277  LogisticRegression    tfidf  \n",
       "303  LogisticRegression    tfidf  \n",
       "281  LogisticRegression    tfidf  \n",
       "319  LogisticRegression    tfidf  \n",
       "307  LogisticRegression    tfidf  \n",
       "309  LogisticRegression    tfidf  \n",
       "283  LogisticRegression    tfidf  \n",
       "321  LogisticRegression    tfidf  \n",
       "314  LogisticRegression    tfidf  \n",
       "320  LogisticRegression    tfidf  \n",
       "299  LogisticRegression    tfidf  \n",
       "306  LogisticRegression    tfidf  \n",
       "289  LogisticRegression    tfidf  \n",
       "293  LogisticRegression    tfidf  \n",
       "312  LogisticRegression    tfidf  \n",
       "302  LogisticRegression    tfidf  \n",
       "310  LogisticRegression    tfidf  \n",
       "291  LogisticRegression    tfidf  \n",
       "290  LogisticRegression    tfidf  \n",
       "294  LogisticRegression    tfidf  \n",
       "315  LogisticRegression    tfidf  \n",
       "278  LogisticRegression    tfidf  \n",
       "301  LogisticRegression    tfidf  \n",
       "308  LogisticRegression    tfidf  \n",
       "313  LogisticRegression    tfidf  \n",
       "279  LogisticRegression    tfidf  \n",
       "282  LogisticRegression    tfidf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_all_intent_classifiers_tfidf_df[report_all_intent_classifiers_tfidf_df['classifier'].str.contains('LogisticRegression')].sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about RF?\n",
    "\n",
    "In some sparse cases it can fair better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.482824</td>\n",
       "      <td>0.239357</td>\n",
       "      <td>0.320051</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>greet</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>24.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>events</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.476987</td>\n",
       "      <td>322.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>querycontact</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.389140</td>\n",
       "      <td>0.510386</td>\n",
       "      <td>221.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>movies</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>112.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>likeness</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.543210</td>\n",
       "      <td>204.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>locations</td>\n",
       "      <td>0.751634</td>\n",
       "      <td>0.449219</td>\n",
       "      <td>0.562347</td>\n",
       "      <td>256.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>convert</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>97.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>hue_lighton</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>38.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>volume_down</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.634483</td>\n",
       "      <td>76.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>factoid</td>\n",
       "      <td>0.639391</td>\n",
       "      <td>0.638783</td>\n",
       "      <td>0.639087</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>maths</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>161.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.606742</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>recipe</td>\n",
       "      <td>0.821875</td>\n",
       "      <td>0.636804</td>\n",
       "      <td>0.717599</td>\n",
       "      <td>413.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>order</td>\n",
       "      <td>0.768362</td>\n",
       "      <td>0.683417</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>199.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>definition</td>\n",
       "      <td>0.652866</td>\n",
       "      <td>0.813492</td>\n",
       "      <td>0.724382</td>\n",
       "      <td>504.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>80.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>hue_lightdim</td>\n",
       "      <td>0.834951</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.751092</td>\n",
       "      <td>126.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>volume_mute</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.662577</td>\n",
       "      <td>0.752613</td>\n",
       "      <td>163.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>createoradd</td>\n",
       "      <td>0.748344</td>\n",
       "      <td>0.768707</td>\n",
       "      <td>0.758389</td>\n",
       "      <td>294.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>game</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.670940</td>\n",
       "      <td>0.758454</td>\n",
       "      <td>234.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>music</td>\n",
       "      <td>0.661719</td>\n",
       "      <td>0.890183</td>\n",
       "      <td>0.759134</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>query</td>\n",
       "      <td>0.699160</td>\n",
       "      <td>0.838851</td>\n",
       "      <td>0.762661</td>\n",
       "      <td>5951.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>volume_up</td>\n",
       "      <td>0.804511</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.772563</td>\n",
       "      <td>144.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>audiobook</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.673640</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>239.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>sendemail</td>\n",
       "      <td>0.695357</td>\n",
       "      <td>0.891147</td>\n",
       "      <td>0.781170</td>\n",
       "      <td>689.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>hue_lightup</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.823944</td>\n",
       "      <td>0.785235</td>\n",
       "      <td>142.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>hue_lightchange</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>224.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>radio</td>\n",
       "      <td>0.867495</td>\n",
       "      <td>0.761818</td>\n",
       "      <td>0.811229</td>\n",
       "      <td>550.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>wemo_off</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>98.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ticket</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>239.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>set</td>\n",
       "      <td>0.835240</td>\n",
       "      <td>0.842470</td>\n",
       "      <td>0.838839</td>\n",
       "      <td>1733.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>post</td>\n",
       "      <td>0.895277</td>\n",
       "      <td>0.805915</td>\n",
       "      <td>0.848249</td>\n",
       "      <td>541.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>cleaning</td>\n",
       "      <td>0.845714</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.853026</td>\n",
       "      <td>172.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>traffic</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>200.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>stock</td>\n",
       "      <td>0.950226</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.855397</td>\n",
       "      <td>270.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>hue_lightoff</td>\n",
       "      <td>0.853755</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.865731</td>\n",
       "      <td>246.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>podcasts</td>\n",
       "      <td>0.949843</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.869440</td>\n",
       "      <td>378.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>remove</td>\n",
       "      <td>0.908696</td>\n",
       "      <td>0.849593</td>\n",
       "      <td>0.878151</td>\n",
       "      <td>984.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>joke</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>122.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>coffee</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.889460</td>\n",
       "      <td>198.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>currency</td>\n",
       "      <td>0.877238</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.892068</td>\n",
       "      <td>378.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>taxi</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.892754</td>\n",
       "      <td>184.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             intent  precision    recall  f1-score  support  \\\n",
       "54      dislikeness   0.375000  0.120000  0.181818     25.0   \n",
       "75           quirky   0.482824  0.239357  0.320051   1057.0   \n",
       "88     volume_other   0.750000  0.250000  0.375000     24.0   \n",
       "81         settings   0.733333  0.275000  0.400000     80.0   \n",
       "58            greet   0.642857  0.375000  0.473684     24.0   \n",
       "55           events   0.730769  0.354037  0.476987    322.0   \n",
       "74     querycontact   0.741379  0.389140  0.510386    221.0   \n",
       "68           movies   0.796296  0.383929  0.518072    112.0   \n",
       "65         likeness   0.733333  0.431373  0.543210    204.0   \n",
       "66        locations   0.751634  0.449219  0.562347    256.0   \n",
       "50          convert   0.880000  0.453608  0.598639     97.0   \n",
       "62      hue_lighton   0.714286  0.526316  0.606061     38.0   \n",
       "86      volume_down   0.666667  0.605263  0.634483     76.0   \n",
       "56          factoid   0.639391  0.638783  0.639087   1052.0   \n",
       "67            maths   0.868687  0.534161  0.661538    161.0   \n",
       "46       addcontact   0.760563  0.606742  0.675000     89.0   \n",
       "77           recipe   0.821875  0.636804  0.717599    413.0   \n",
       "70            order   0.768362  0.683417  0.723404    199.0   \n",
       "53       definition   0.652866  0.813492  0.724382    504.0   \n",
       "91          wemo_on   0.808824  0.687500  0.743243     80.0   \n",
       "60     hue_lightdim   0.834951  0.682540  0.751092    126.0   \n",
       "87      volume_mute   0.870968  0.662577  0.752613    163.0   \n",
       "51      createoradd   0.748344  0.768707  0.758389    294.0   \n",
       "57             game   0.872222  0.670940  0.758454    234.0   \n",
       "69            music   0.661719  0.890183  0.759134   1202.0   \n",
       "73            query   0.699160  0.838851  0.762661   5951.0   \n",
       "89        volume_up   0.804511  0.743056  0.772563    144.0   \n",
       "47        audiobook   0.909605  0.673640  0.774038    239.0   \n",
       "79        sendemail   0.695357  0.891147  0.781170    689.0   \n",
       "63      hue_lightup   0.750000  0.823944  0.785235    142.0   \n",
       "59  hue_lightchange   0.808036  0.808036  0.808036    224.0   \n",
       "76            radio   0.867495  0.761818  0.811229    550.0   \n",
       "90         wemo_off   0.843750  0.826531  0.835052     98.0   \n",
       "84           ticket   0.836820  0.836820  0.836820    239.0   \n",
       "80              set   0.835240  0.842470  0.838839   1733.0   \n",
       "72             post   0.895277  0.805915  0.848249    541.0   \n",
       "48         cleaning   0.845714  0.860465  0.853026    172.0   \n",
       "85          traffic   0.968354  0.765000  0.854749    200.0   \n",
       "82            stock   0.950226  0.777778  0.855397    270.0   \n",
       "61     hue_lightoff   0.853755  0.878049  0.865731    246.0   \n",
       "71         podcasts   0.949843  0.801587  0.869440    378.0   \n",
       "78           remove   0.908696  0.849593  0.878151    984.0   \n",
       "64             joke   0.927273  0.836066  0.879310    122.0   \n",
       "49           coffee   0.905759  0.873737  0.889460    198.0   \n",
       "52         currency   0.877238  0.907407  0.892068    378.0   \n",
       "83             taxi   0.956522  0.836957  0.892754    184.0   \n",
       "\n",
       "                classifier encoding  \n",
       "54  RandomForestClassifier    tfidf  \n",
       "75  RandomForestClassifier    tfidf  \n",
       "88  RandomForestClassifier    tfidf  \n",
       "81  RandomForestClassifier    tfidf  \n",
       "58  RandomForestClassifier    tfidf  \n",
       "55  RandomForestClassifier    tfidf  \n",
       "74  RandomForestClassifier    tfidf  \n",
       "68  RandomForestClassifier    tfidf  \n",
       "65  RandomForestClassifier    tfidf  \n",
       "66  RandomForestClassifier    tfidf  \n",
       "50  RandomForestClassifier    tfidf  \n",
       "62  RandomForestClassifier    tfidf  \n",
       "86  RandomForestClassifier    tfidf  \n",
       "56  RandomForestClassifier    tfidf  \n",
       "67  RandomForestClassifier    tfidf  \n",
       "46  RandomForestClassifier    tfidf  \n",
       "77  RandomForestClassifier    tfidf  \n",
       "70  RandomForestClassifier    tfidf  \n",
       "53  RandomForestClassifier    tfidf  \n",
       "91  RandomForestClassifier    tfidf  \n",
       "60  RandomForestClassifier    tfidf  \n",
       "87  RandomForestClassifier    tfidf  \n",
       "51  RandomForestClassifier    tfidf  \n",
       "57  RandomForestClassifier    tfidf  \n",
       "69  RandomForestClassifier    tfidf  \n",
       "73  RandomForestClassifier    tfidf  \n",
       "89  RandomForestClassifier    tfidf  \n",
       "47  RandomForestClassifier    tfidf  \n",
       "79  RandomForestClassifier    tfidf  \n",
       "63  RandomForestClassifier    tfidf  \n",
       "59  RandomForestClassifier    tfidf  \n",
       "76  RandomForestClassifier    tfidf  \n",
       "90  RandomForestClassifier    tfidf  \n",
       "84  RandomForestClassifier    tfidf  \n",
       "80  RandomForestClassifier    tfidf  \n",
       "72  RandomForestClassifier    tfidf  \n",
       "48  RandomForestClassifier    tfidf  \n",
       "85  RandomForestClassifier    tfidf  \n",
       "82  RandomForestClassifier    tfidf  \n",
       "61  RandomForestClassifier    tfidf  \n",
       "71  RandomForestClassifier    tfidf  \n",
       "78  RandomForestClassifier    tfidf  \n",
       "64  RandomForestClassifier    tfidf  \n",
       "49  RandomForestClassifier    tfidf  \n",
       "52  RandomForestClassifier    tfidf  \n",
       "83  RandomForestClassifier    tfidf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_all_intent_classifiers_tfidf_df[report_all_intent_classifiers_tfidf_df['classifier'].str.contains('RandomForest')].sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'dislikeness' scores pretty poorly with every classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>SVC</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>25.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>25.0</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>25.0</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          intent  precision  recall  f1-score  support  \\\n",
       "238  dislikeness   0.857143    0.24  0.375000     25.0   \n",
       "330  dislikeness   0.555556    0.20  0.294118     25.0   \n",
       "54   dislikeness   0.375000    0.12  0.181818     25.0   \n",
       "100  dislikeness   0.130435    0.12  0.125000     25.0   \n",
       "8    dislikeness   0.056000    0.28  0.093333     25.0   \n",
       "146  dislikeness   0.025974    0.08  0.039216     25.0   \n",
       "192  dislikeness   0.000000    0.00  0.000000     25.0   \n",
       "284  dislikeness   0.000000    0.00  0.000000     25.0   \n",
       "\n",
       "                 classifier encoding  \n",
       "238                     SVC    tfidf  \n",
       "330           XGBClassifier    tfidf  \n",
       "54   RandomForestClassifier    tfidf  \n",
       "100  DecisionTreeClassifier    tfidf  \n",
       "8                GaussianNB    tfidf  \n",
       "146      AdaBoostClassifier    tfidf  \n",
       "192    KNeighborsClassifier    tfidf  \n",
       "284      LogisticRegression    tfidf  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_all_intent_classifiers_tfidf_df[report_all_intent_classifiers_tfidf_df['intent'].str.contains('dislikeness')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for 'quirky'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>0.328288</td>\n",
       "      <td>0.398621</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>SVC</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.463115</td>\n",
       "      <td>0.338323</td>\n",
       "      <td>0.391003</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.425422</td>\n",
       "      <td>0.327345</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.482824</td>\n",
       "      <td>0.239357</td>\n",
       "      <td>0.320051</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.281407</td>\n",
       "      <td>0.264901</td>\n",
       "      <td>0.272904</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.191565</td>\n",
       "      <td>0.253548</td>\n",
       "      <td>0.218241</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.047304</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     intent  precision    recall  f1-score  support              classifier  \\\n",
       "259  quirky   0.507310  0.328288  0.398621   1057.0                     SVC   \n",
       "351  quirky   0.463115  0.338323  0.391003   1002.0           XGBClassifier   \n",
       "305  quirky   0.425422  0.327345  0.369994   1002.0      LogisticRegression   \n",
       "75   quirky   0.482824  0.239357  0.320051   1057.0  RandomForestClassifier   \n",
       "121  quirky   0.281407  0.264901  0.272904   1057.0  DecisionTreeClassifier   \n",
       "29   quirky   0.191565  0.253548  0.218241   1057.0              GaussianNB   \n",
       "213  quirky   0.458716  0.047304  0.085763   1057.0    KNeighborsClassifier   \n",
       "167  quirky   0.000000  0.000000  0.000000   1057.0      AdaBoostClassifier   \n",
       "\n",
       "    encoding  \n",
       "259    tfidf  \n",
       "351    tfidf  \n",
       "305    tfidf  \n",
       "75     tfidf  \n",
       "121    tfidf  \n",
       "29     tfidf  \n",
       "213    tfidf  \n",
       "167    tfidf  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_all_intent_classifiers_tfidf_df[report_all_intent_classifiers_tfidf_df['intent'].str.contains('quirky')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'volume_other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SVC</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>24.0</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>24.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>24.0</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           intent  precision    recall  f1-score  support  \\\n",
       "272  volume_other   0.750000  0.375000  0.500000     24.0   \n",
       "134  volume_other   0.615385  0.333333  0.432432     24.0   \n",
       "364  volume_other   0.473684  0.375000  0.418605     24.0   \n",
       "88   volume_other   0.750000  0.250000  0.375000     24.0   \n",
       "42   volume_other   0.179487  0.291667  0.222222     24.0   \n",
       "180  volume_other   0.000000  0.000000  0.000000     24.0   \n",
       "226  volume_other   0.000000  0.000000  0.000000     24.0   \n",
       "318  volume_other   0.000000  0.000000  0.000000     24.0   \n",
       "\n",
       "                 classifier encoding  \n",
       "272                     SVC    tfidf  \n",
       "134  DecisionTreeClassifier    tfidf  \n",
       "364           XGBClassifier    tfidf  \n",
       "88   RandomForestClassifier    tfidf  \n",
       "42               GaussianNB    tfidf  \n",
       "180      AdaBoostClassifier    tfidf  \n",
       "226    KNeighborsClassifier    tfidf  \n",
       "318      LogisticRegression    tfidf  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_all_intent_classifiers_tfidf_df[report_all_intent_classifiers_tfidf_df['intent'].str.contains('volume_other')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'settings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>80.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>80.0</td>\n",
       "      <td>SVC</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>80.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>80.0</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.131818</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>80.0</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>80.0</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       intent  precision  recall  f1-score  support              classifier  \\\n",
       "357  settings   0.678571  0.4750  0.558824     80.0           XGBClassifier   \n",
       "265  settings   0.790698  0.4250  0.552846     80.0                     SVC   \n",
       "311  settings   0.866667  0.3250  0.472727     80.0      LogisticRegression   \n",
       "81   settings   0.733333  0.2750  0.400000     80.0  RandomForestClassifier   \n",
       "127  settings   0.370370  0.3750  0.372671     80.0  DecisionTreeClassifier   \n",
       "35   settings   0.131818  0.3625  0.193333     80.0              GaussianNB   \n",
       "219  settings   0.500000  0.0125  0.024390     80.0    KNeighborsClassifier   \n",
       "173  settings   0.000000  0.0000  0.000000     80.0      AdaBoostClassifier   \n",
       "\n",
       "    encoding  \n",
       "357    tfidf  \n",
       "265    tfidf  \n",
       "311    tfidf  \n",
       "81     tfidf  \n",
       "127    tfidf  \n",
       "35     tfidf  \n",
       "219    tfidf  \n",
       "173    tfidf  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_all_intent_classifiers_tfidf_df[report_all_intent_classifiers_tfidf_df['intent'].str.contains('settings')].sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which classifiers score the best for each intent, arranged by support (how sparse the intent examples are)\n",
    "Note: this excludes SVM (due to run time), but generally SVM would score the best.\n",
    "\n",
    "Skip the next cell, it is there just to reproduce the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_best_classifier_per_intent_tfidf_df = report_all_intent_classifiers_tfidf_df[~report_all_intent_classifiers_tfidf_df['classifier'].str.contains('SVC')].groupby('intent').apply(lambda x: x.sort_values(by=['f1-score'], ascending=False).head(1)).sort_values(by=['support'])\n",
    "report_best_classifier_per_intent_tfidf_df.to_csv('report_best_classifier_per_intent_tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume_other</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>24.0</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greet</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>24.0</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dislikeness</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>25.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hue_lighton</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>38.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>volume_down</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>73.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>80.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>settings</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>80.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.606742</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>convert</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>97.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wemo_off</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>98.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>movies</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>109.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>joke</td>\n",
       "      <td>0.955357</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.918455</td>\n",
       "      <td>121.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hue_lightdim</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.863436</td>\n",
       "      <td>125.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hue_lightup</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>141.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>volume_up</td>\n",
       "      <td>0.804511</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.772563</td>\n",
       "      <td>144.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>maths</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.554140</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>157.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>volume_mute</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.662577</td>\n",
       "      <td>0.752613</td>\n",
       "      <td>163.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cleaning</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>172.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>taxi</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.935933</td>\n",
       "      <td>183.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>traffic</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>194.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>coffee</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>197.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>order</td>\n",
       "      <td>0.768362</td>\n",
       "      <td>0.683417</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>199.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>likeness</td>\n",
       "      <td>0.679739</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.589235</td>\n",
       "      <td>200.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>querycontact</td>\n",
       "      <td>0.654054</td>\n",
       "      <td>0.552511</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>219.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hue_lightchange</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.807175</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>223.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>game</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.670940</td>\n",
       "      <td>0.758454</td>\n",
       "      <td>234.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>audiobook</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.673640</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>239.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ticket</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>239.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hue_lightoff</td>\n",
       "      <td>0.906383</td>\n",
       "      <td>0.869388</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>245.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>locations</td>\n",
       "      <td>0.841772</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.642512</td>\n",
       "      <td>256.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>stock</td>\n",
       "      <td>0.947598</td>\n",
       "      <td>0.806691</td>\n",
       "      <td>0.871486</td>\n",
       "      <td>269.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>createoradd</td>\n",
       "      <td>0.748344</td>\n",
       "      <td>0.768707</td>\n",
       "      <td>0.758389</td>\n",
       "      <td>294.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>events</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.468553</td>\n",
       "      <td>0.599598</td>\n",
       "      <td>318.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>podcasts</td>\n",
       "      <td>0.986971</td>\n",
       "      <td>0.816712</td>\n",
       "      <td>0.893805</td>\n",
       "      <td>371.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>currency</td>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.930612</td>\n",
       "      <td>378.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>recipe</td>\n",
       "      <td>0.845029</td>\n",
       "      <td>0.711823</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>406.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>definition</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.817610</td>\n",
       "      <td>490.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>post</td>\n",
       "      <td>0.895277</td>\n",
       "      <td>0.805915</td>\n",
       "      <td>0.848249</td>\n",
       "      <td>541.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>radio</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.773481</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>543.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sendemail</td>\n",
       "      <td>0.695357</td>\n",
       "      <td>0.891147</td>\n",
       "      <td>0.781170</td>\n",
       "      <td>689.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>remove</td>\n",
       "      <td>0.964450</td>\n",
       "      <td>0.862564</td>\n",
       "      <td>0.910666</td>\n",
       "      <td>975.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>quirky</td>\n",
       "      <td>0.463115</td>\n",
       "      <td>0.338323</td>\n",
       "      <td>0.391003</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>factoid</td>\n",
       "      <td>0.689011</td>\n",
       "      <td>0.601727</td>\n",
       "      <td>0.642418</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>music</td>\n",
       "      <td>0.780895</td>\n",
       "      <td>0.871404</td>\n",
       "      <td>0.823671</td>\n",
       "      <td>1182.0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>set</td>\n",
       "      <td>0.844318</td>\n",
       "      <td>0.863953</td>\n",
       "      <td>0.854023</td>\n",
       "      <td>1720.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>query</td>\n",
       "      <td>0.699160</td>\n",
       "      <td>0.838851</td>\n",
       "      <td>0.762661</td>\n",
       "      <td>5951.0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             intent  precision    recall  f1-score  support  \\\n",
       "0      volume_other   0.615385  0.333333  0.432432     24.0   \n",
       "1             greet   0.555556  0.416667  0.476190     24.0   \n",
       "2       dislikeness   0.555556  0.200000  0.294118     25.0   \n",
       "3       hue_lighton   0.714286  0.526316  0.606061     38.0   \n",
       "4       volume_down   0.916667  0.602740  0.727273     73.0   \n",
       "5           wemo_on   0.981481  0.662500  0.791045     80.0   \n",
       "6          settings   0.678571  0.475000  0.558824     80.0   \n",
       "7        addcontact   0.760563  0.606742  0.675000     89.0   \n",
       "8           convert   0.794118  0.556701  0.654545     97.0   \n",
       "9          wemo_off   0.843750  0.826531  0.835052     98.0   \n",
       "10           movies   0.841270  0.486239  0.616279    109.0   \n",
       "11             joke   0.955357  0.884298  0.918455    121.0   \n",
       "12     hue_lightdim   0.960784  0.784000  0.863436    125.0   \n",
       "13      hue_lightup   0.902439  0.787234  0.840909    141.0   \n",
       "14        volume_up   0.804511  0.743056  0.772563    144.0   \n",
       "15            maths   0.935484  0.554140  0.696000    157.0   \n",
       "16      volume_mute   0.870968  0.662577  0.752613    163.0   \n",
       "17         cleaning   0.985714  0.802326  0.884615    172.0   \n",
       "18             taxi   0.954545  0.918033  0.935933    183.0   \n",
       "19          traffic   0.968553  0.793814  0.872521    194.0   \n",
       "20           coffee   0.988304  0.857868  0.918478    197.0   \n",
       "21            order   0.768362  0.683417  0.723404    199.0   \n",
       "22         likeness   0.679739  0.520000  0.589235    200.0   \n",
       "23     querycontact   0.654054  0.552511  0.599010    219.0   \n",
       "24  hue_lightchange   0.873786  0.807175  0.839161    223.0   \n",
       "25             game   0.872222  0.670940  0.758454    234.0   \n",
       "26        audiobook   0.909605  0.673640  0.774038    239.0   \n",
       "27           ticket   0.836820  0.836820  0.836820    239.0   \n",
       "28     hue_lightoff   0.906383  0.869388  0.887500    245.0   \n",
       "29        locations   0.841772  0.519531  0.642512    256.0   \n",
       "30            stock   0.947598  0.806691  0.871486    269.0   \n",
       "31      createoradd   0.748344  0.768707  0.758389    294.0   \n",
       "32           events   0.832402  0.468553  0.599598    318.0   \n",
       "33         podcasts   0.986971  0.816712  0.893805    371.0   \n",
       "34         currency   0.957983  0.904762  0.930612    378.0   \n",
       "35           recipe   0.845029  0.711823  0.772727    406.0   \n",
       "36       definition   0.840517  0.795918  0.817610    490.0   \n",
       "37             post   0.895277  0.805915  0.848249    541.0   \n",
       "38            radio   0.909091  0.773481  0.835821    543.0   \n",
       "39        sendemail   0.695357  0.891147  0.781170    689.0   \n",
       "40           remove   0.964450  0.862564  0.910666    975.0   \n",
       "41           quirky   0.463115  0.338323  0.391003   1002.0   \n",
       "42          factoid   0.689011  0.601727  0.642418   1042.0   \n",
       "43            music   0.780895  0.871404  0.823671   1182.0   \n",
       "44              set   0.844318  0.863953  0.854023   1720.0   \n",
       "45            query   0.699160  0.838851  0.762661   5951.0   \n",
       "\n",
       "                classifier encoding  \n",
       "0   DecisionTreeClassifier    tfidf  \n",
       "1   DecisionTreeClassifier    tfidf  \n",
       "2            XGBClassifier    tfidf  \n",
       "3   RandomForestClassifier    tfidf  \n",
       "4       LogisticRegression    tfidf  \n",
       "5       LogisticRegression    tfidf  \n",
       "6            XGBClassifier    tfidf  \n",
       "7   RandomForestClassifier    tfidf  \n",
       "8            XGBClassifier    tfidf  \n",
       "9   RandomForestClassifier    tfidf  \n",
       "10      LogisticRegression    tfidf  \n",
       "11           XGBClassifier    tfidf  \n",
       "12      LogisticRegression    tfidf  \n",
       "13      LogisticRegression    tfidf  \n",
       "14  RandomForestClassifier    tfidf  \n",
       "15      LogisticRegression    tfidf  \n",
       "16  RandomForestClassifier    tfidf  \n",
       "17      LogisticRegression    tfidf  \n",
       "18           XGBClassifier    tfidf  \n",
       "19      LogisticRegression    tfidf  \n",
       "20      LogisticRegression    tfidf  \n",
       "21  RandomForestClassifier    tfidf  \n",
       "22           XGBClassifier    tfidf  \n",
       "23           XGBClassifier    tfidf  \n",
       "24      LogisticRegression    tfidf  \n",
       "25  RandomForestClassifier    tfidf  \n",
       "26  RandomForestClassifier    tfidf  \n",
       "27  RandomForestClassifier    tfidf  \n",
       "28           XGBClassifier    tfidf  \n",
       "29      LogisticRegression    tfidf  \n",
       "30           XGBClassifier    tfidf  \n",
       "31  RandomForestClassifier    tfidf  \n",
       "32      LogisticRegression    tfidf  \n",
       "33      LogisticRegression    tfidf  \n",
       "34      LogisticRegression    tfidf  \n",
       "35           XGBClassifier    tfidf  \n",
       "36           XGBClassifier    tfidf  \n",
       "37  RandomForestClassifier    tfidf  \n",
       "38      LogisticRegression    tfidf  \n",
       "39  RandomForestClassifier    tfidf  \n",
       "40           XGBClassifier    tfidf  \n",
       "41           XGBClassifier    tfidf  \n",
       "42      LogisticRegression    tfidf  \n",
       "43           XGBClassifier    tfidf  \n",
       "44      LogisticRegression    tfidf  \n",
       "45  RandomForestClassifier    tfidf  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_best_classifier_per_intent_tfidf_df = pd.read_csv('report_best_classifier_per_intent_tfidf.csv')\n",
    "report_best_classifier_per_intent_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression        17\n",
       "XGBClassifier             14\n",
       "RandomForestClassifier    13\n",
       "DecisionTreeClassifier     2\n",
       "Name: classifier, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_best_classifier_per_intent_tfidf_df['classifier'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "* It is clear to see that some of the utterances are poorly written or otherwise incorrect and some of the intents are overlapping\n",
    "* I think on a real data set you could see at least ~10% improved performance\n",
    "* There are a lot of rules that can be addeded between the two classifiers (intent and entity tagging), that could boost the model, in addition to fine tuning the model itself.\n",
    "\n",
    "It is my opinion that a random forest or logistic regression with TFIDF and a CRF entity tagger would work fine for NLU tasks, even on under-powered devices, including TRAINING!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For TFIDF the winner is LR (except for sparse data), followed by XGB and RF!\n",
    "Unless pure performance is your goal, then SVM for the win. But it is nice to balance out performance vs speed (training and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We shall use RF as an example classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = train_classifier(RF, tfidf_utterances_vectors, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the intent label from an utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(classifier_model, utterance):\n",
    "    utterance = utterance.lower()\n",
    "    transformed_utterance = vectorizer.transform([utterance])\n",
    "    predicted_label = classifier_model.predict(transformed_utterance)\n",
    "    return decode_labels(predicted_label)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out yourself with an utterance\n",
    "utterance = 'Turn the living room lights off'\n",
    "label = predict_label(RF_model, utterance)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it get wrong and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrectly_classified_utterances(classifier_model, utterances, tfidf_utterances_vectors, label_encoded_y):\n",
    "    y_prediction = classifier_model.predict(tfidf_utterances_vectors)\n",
    "    for utterance, prediction, intent in zip(utterances, decode_labels(y_prediction), decode_labels(label_encoded_y)):\n",
    "        if str(prediction) not in str(intent):\n",
    "            print(f'{utterance} has been classified as {prediction}, but it should be {intent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df[(nlu_data_df['answer_normalised'].str.fullmatch('a')) & (nlu_data_df['intent'].str.contains('factoid'))]\n",
    "# TODO: for future cleaning we can for sure get rid of the following:\n",
    "#answer id: 19126.0, 21940.0, 21942.0, 25765.0, 4274.0\n",
    "# go by user ID too? ie 981.0, 107.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_incorrectly_classified_utterances(RF_model, nlu_data_df['answer_normalised'].tolist(), tfidf_utterances_vectors, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it very clear that the data set is not clean enough to ensure good results. A future cleanup will be required (also perhaps seperating the intents a bit better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the entities from the utterances with their taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_types_and_entities(entities):\n",
    "    entity_list = []\n",
    "    for entity in entities:\n",
    "        split_entity = entity.split(' : ')\n",
    "        entity_type = split_entity[0]\n",
    "        entity_text = split_entity[1].split(' ')\n",
    "        entity_list.append({'type':entity_type, 'words': entity_text})\n",
    "    return entity_list\n",
    "\n",
    "def extract_entities(utterance):\n",
    "    entities = re.findall(r'\\[(.*?)\\]', utterance)\n",
    "    return seperate_types_and_entities(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_with_tagging = 'wake me up at [time : five pm] [date : this week]'\n",
    "\n",
    "entities = extract_entities(utterance_with_tagging)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging and entity labeling of utterances\n",
    "Conditional random fields just love features. One of the most obvious features we could give it besides the words themselves are the part of speech (POS) tags of the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_utterance(utterance):\n",
    "    tokenized_utterance = nltk.word_tokenize(utterance)\n",
    "    utterance_pos = nltk.pos_tag(tokenized_utterance)\n",
    "    return utterance_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = 'wake me up at five pm this week'\n",
    "utterance_pos = pos_tag_utterance(utterance)\n",
    "utterance_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pos_and_entity_tags(entities, utterance_pos):\n",
    "    output = []\n",
    "    words = []\n",
    "\n",
    "    for entity in entities:\n",
    "        for word in entity['words']:\n",
    "            words.append(word)\n",
    "\n",
    "    for pair in utterance_pos:\n",
    "        word = pair[0]\n",
    "        pos = pair[1]\n",
    "        for entity in entities:\n",
    "            if word in entity['words']:\n",
    "                entity_type = entity['type']\n",
    "                output.append((word, pos, entity_type))\n",
    "            elif word not in words and entity is entities[-1]:\n",
    "                entity_type = '0'\n",
    "                output.append((word, pos, entity_type))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_pos_and_entity_tags(entities, utterance_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's put it all together to make our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dataset(nlu_data_df):\n",
    "    feature_dataset = []\n",
    "    for utterance, utterance_with_tagging in zip(nlu_data_df['answer_normalised'], nlu_data_df['answer_annotation']):\n",
    "        print(utterance)\n",
    "        entities = extract_entities(utterance_with_tagging)\n",
    "        utterance_pos = pos_tag_utterance(utterance)\n",
    "        feature_dataset.append(combine_pos_and_entity_tags(entities, utterance_pos))\n",
    "    return feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try removing the word-level slicing (I don't think prefixes and suffixes give more info in English)\n",
    "# TODO: try add stemming (lemma?) or something as an extra feature?\n",
    "def word2features(utterance, i):\n",
    "    word = utterance[i][0]\n",
    "    postag = utterance[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = utterance[i-1][0]\n",
    "        postag1 = utterance[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word': word1,\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(utterance)-1:\n",
    "        word1 = utterance[i+1][0]\n",
    "        postag1 = utterance[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word': word1,\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def utterance2features(utterance):\n",
    "    return [word2features(utterance, i) for i in range(len(utterance))]\n",
    "\n",
    "def utterance2labels(utterance):\n",
    "    return [label for token, postag, label in utterance]\n",
    "\n",
    "def utterance2tokens(utterance):\n",
    "    return [token for token, postag, label in utterance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: it is easy to see that for entities with few examples, the results are very poor.\n",
    "\n",
    "Unless you want to reproduce the report, you can skip the next 3 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = create_feature_dataset(nlu_data_df)\n",
    "feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [utterance2features(utterance) for utterance in feature_dataset]\n",
    "y = [utterance2labels(utterance) for utterance in feature_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "report = flat_classification_report(y_pred=pred, y_true=y, output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.index = df.index.set_names(['entity-type'])\n",
    "df = df.reset_index()\n",
    "df.to_csv('analysis_of_CRF_for_entity_extraction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_CRF_entity_extraction_df = pd.read_csv('analysis_of_CRF_for_entity_extraction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally our CRF performs poorly for entities with few examples. This could be optimized by using additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity-type</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.889049</td>\n",
       "      <td>0.958075</td>\n",
       "      <td>0.922272</td>\n",
       "      <td>76803.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>currency_name</td>\n",
       "      <td>0.943467</td>\n",
       "      <td>0.876313</td>\n",
       "      <td>0.908651</td>\n",
       "      <td>857.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>timeofday</td>\n",
       "      <td>0.860370</td>\n",
       "      <td>0.918860</td>\n",
       "      <td>0.888653</td>\n",
       "      <td>456.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.844292</td>\n",
       "      <td>0.844292</td>\n",
       "      <td>0.844292</td>\n",
       "      <td>0.844292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>date</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.846817</td>\n",
       "      <td>0.838315</td>\n",
       "      <td>4289.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.816216</td>\n",
       "      <td>0.844292</td>\n",
       "      <td>0.826035</td>\n",
       "      <td>106141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>house_place</td>\n",
       "      <td>0.854592</td>\n",
       "      <td>0.793839</td>\n",
       "      <td>0.823096</td>\n",
       "      <td>422.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>device_type</td>\n",
       "      <td>0.810845</td>\n",
       "      <td>0.818066</td>\n",
       "      <td>0.814440</td>\n",
       "      <td>786.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>time</td>\n",
       "      <td>0.803602</td>\n",
       "      <td>0.790729</td>\n",
       "      <td>0.797114</td>\n",
       "      <td>2934.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>meal_type</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>music_genre</td>\n",
       "      <td>0.743523</td>\n",
       "      <td>0.747396</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>384.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>color_type</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.741214</td>\n",
       "      <td>174.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>joke_type</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>relation</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.685039</td>\n",
       "      <td>434.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>order_type</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.572864</td>\n",
       "      <td>0.672566</td>\n",
       "      <td>199.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alarm_type</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>app_name</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>person</td>\n",
       "      <td>0.688588</td>\n",
       "      <td>0.576985</td>\n",
       "      <td>0.627866</td>\n",
       "      <td>1851.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>place_name</td>\n",
       "      <td>0.616919</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.620439</td>\n",
       "      <td>2875.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>list_name</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.475352</td>\n",
       "      <td>0.580021</td>\n",
       "      <td>568.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>weather_descriptor</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>0.461752</td>\n",
       "      <td>0.569957</td>\n",
       "      <td>719.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>change_amount</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.564315</td>\n",
       "      <td>162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artist_name</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.504839</td>\n",
       "      <td>0.558929</td>\n",
       "      <td>620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>event_name</td>\n",
       "      <td>0.538782</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.542993</td>\n",
       "      <td>2602.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>general_frequency</td>\n",
       "      <td>0.629834</td>\n",
       "      <td>0.448819</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>254.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business_type</td>\n",
       "      <td>0.742690</td>\n",
       "      <td>0.346995</td>\n",
       "      <td>0.472998</td>\n",
       "      <td>366.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>food_type</td>\n",
       "      <td>0.615012</td>\n",
       "      <td>0.347945</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>730.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>time_zone</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.443350</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business_name</td>\n",
       "      <td>0.533929</td>\n",
       "      <td>0.336712</td>\n",
       "      <td>0.412983</td>\n",
       "      <td>888.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>definition_word</td>\n",
       "      <td>0.487324</td>\n",
       "      <td>0.336576</td>\n",
       "      <td>0.398159</td>\n",
       "      <td>514.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>playlist_name</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.291457</td>\n",
       "      <td>0.387960</td>\n",
       "      <td>199.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>media_type</td>\n",
       "      <td>0.625616</td>\n",
       "      <td>0.264583</td>\n",
       "      <td>0.371889</td>\n",
       "      <td>960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.441572</td>\n",
       "      <td>0.322593</td>\n",
       "      <td>0.360695</td>\n",
       "      <td>106141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>player_setting</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.343558</td>\n",
       "      <td>343.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>coffee_type</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>news_topic</td>\n",
       "      <td>0.272381</td>\n",
       "      <td>0.234811</td>\n",
       "      <td>0.252205</td>\n",
       "      <td>609.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>transport_agency</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>song_name</td>\n",
       "      <td>0.261161</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.232373</td>\n",
       "      <td>559.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>music_descriptor</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audiobook_name</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>personal_info</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>podcast_name</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>transport_type</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>email_folder</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cooking_type</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audiobook_author</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>transport_name</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>transport_descriptor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>drink_type</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>email_address</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>game_name</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>music_album</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>sport_type</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>game_type</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ingredient</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>radio_name</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>737.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>query_detail</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>podcast_descriptor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>movie_type</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>movie_name</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             entity-type  precision    recall  f1-score        support\n",
       "0                      0   0.889049  0.958075  0.922272   76803.000000\n",
       "12         currency_name   0.943467  0.876313  0.908651     857.000000\n",
       "51             timeofday   0.860370  0.918860  0.888653     456.000000\n",
       "57              accuracy   0.844292  0.844292  0.844292       0.844292\n",
       "13                  date   0.829982  0.846817  0.838315    4289.000000\n",
       "59          weighted avg   0.816216  0.844292  0.826035  106141.000000\n",
       "24           house_place   0.854592  0.793839  0.823096     422.000000\n",
       "15           device_type   0.810845  0.818066  0.814440     786.000000\n",
       "49                  time   0.803602  0.790729  0.797114    2934.000000\n",
       "28             meal_type   0.754545  0.747748  0.751131     111.000000\n",
       "34           music_genre   0.743523  0.747396  0.745455     384.000000\n",
       "10            color_type   0.834532  0.666667  0.741214     174.000000\n",
       "26             joke_type   0.884615  0.613333  0.724409      75.000000\n",
       "46              relation   0.795732  0.601382  0.685039     434.000000\n",
       "36            order_type   0.814286  0.572864  0.672566     199.000000\n",
       "1             alarm_type   0.800000  0.533333  0.640000      30.000000\n",
       "2               app_name   0.812500  0.527027  0.639344      74.000000\n",
       "37                person   0.688588  0.576985  0.627866    1851.000000\n",
       "39            place_name   0.616919  0.624000  0.620439    2875.000000\n",
       "27             list_name   0.743802  0.475352  0.580021     568.000000\n",
       "56    weather_descriptor   0.744395  0.461752  0.569957     719.000000\n",
       "8          change_amount   0.860759  0.419753  0.564315     162.000000\n",
       "3            artist_name   0.626000  0.504839  0.558929     620.000000\n",
       "19            event_name   0.538782  0.547271  0.542993    2602.000000\n",
       "23     general_frequency   0.629834  0.448819  0.524138     254.000000\n",
       "7          business_type   0.742690  0.346995  0.472998     366.000000\n",
       "20             food_type   0.615012  0.347945  0.444444     730.000000\n",
       "50             time_zone   0.692308  0.326087  0.443350     138.000000\n",
       "6          business_name   0.533929  0.336712  0.412983     888.000000\n",
       "14       definition_word   0.487324  0.336576  0.398159     514.000000\n",
       "41         playlist_name   0.580000  0.291457  0.387960     199.000000\n",
       "29            media_type   0.625616  0.264583  0.371889     960.000000\n",
       "58             macro avg   0.441572  0.322593  0.360695  106141.000000\n",
       "40        player_setting   0.575342  0.244898  0.343558     343.000000\n",
       "9            coffee_type   0.800000  0.163265  0.271186      49.000000\n",
       "35            news_topic   0.272381  0.234811  0.252205     609.000000\n",
       "52      transport_agency   0.714286  0.142857  0.238095      70.000000\n",
       "47             song_name   0.261161  0.209302  0.232373     559.000000\n",
       "33      music_descriptor   0.116279  0.034722  0.053476     144.000000\n",
       "5         audiobook_name   0.038462  0.014493  0.021053     276.000000\n",
       "38         personal_info   0.100000  0.009662  0.017621     207.000000\n",
       "43          podcast_name   0.022989  0.010363  0.014286     193.000000\n",
       "55        transport_type   0.111111  0.001838  0.003617     544.000000\n",
       "18          email_folder   0.000000  0.000000  0.000000       3.000000\n",
       "11          cooking_type   0.000000  0.000000  0.000000      23.000000\n",
       "4       audiobook_author   0.000000  0.000000  0.000000      28.000000\n",
       "54        transport_name   0.000000  0.000000  0.000000      50.000000\n",
       "53  transport_descriptor   0.000000  0.000000  0.000000      34.000000\n",
       "16            drink_type   0.000000  0.000000  0.000000      22.000000\n",
       "17         email_address   0.000000  0.000000  0.000000      30.000000\n",
       "21             game_name   0.000000  0.000000  0.000000     318.000000\n",
       "32           music_album   0.000000  0.000000  0.000000       8.000000\n",
       "48            sport_type   0.000000  0.000000  0.000000      16.000000\n",
       "22             game_type   0.000000  0.000000  0.000000       3.000000\n",
       "25            ingredient   0.000000  0.000000  0.000000      52.000000\n",
       "45            radio_name   0.000000  0.000000  0.000000     737.000000\n",
       "44          query_detail   0.000000  0.000000  0.000000       7.000000\n",
       "42    podcast_descriptor   0.000000  0.000000  0.000000     268.000000\n",
       "31            movie_type   0.000000  0.000000  0.000000      24.000000\n",
       "30            movie_name   0.000000  0.000000  0.000000      50.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_CRF_entity_extraction_df.sort_values(by=['f1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the entities with the fewest examples and the others that score 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_strings = ['audiobook_author', 'audiobook_name', 'cooking_type', 'drink_type', 'email_address', 'email_folder', 'game_name', 'game_type', 'ingredient', 'movie_name', 'movie_type', 'music_album', 'music_descriptor', 'news_topic', 'personal_info', 'podcast_descriptor', 'podcast_name', 'query_detail', 'radio_name', 'song_name', 'sport_type', 'transport_descriptor', 'transport_name', 'transport_type']\n",
    "\n",
    "nlu_data_entities_cleaned_df = nlu_data_df[~nlu_data_df['answer_annotation'].str.contains('|'.join(remove_strings))]\n",
    "nlu_data_entities_cleaned_df\n",
    "# TODO: fix the one entity type label with no space after entity type (ie type: thing -> type : thing)\n",
    "# TODO: change nlu_data_df to cleaned for rest of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = create_feature_dataset(nlu_data_entities_cleaned_df)\n",
    "X = [utterance2features(utterance) for utterance in feature_dataset]\n",
    "y = [utterance2labels(utterance) for utterance in feature_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(algorithm='lbfgs',\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we want to use this as an entity extraction engine, we will need to get the entities, their types, and the location of the entities in the utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(utterance):\n",
    "    utterance_pos = pos_tag_utterance(utterance)\n",
    "    utterance_features = utterance2features(utterance_pos)\n",
    "    label = crf_model.predict_single(utterance_features)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_types_and_locations(utterance):\n",
    "    entity_locations_and_types = []\n",
    "    entities = get_entities(utterance)\n",
    "    for location, entity in enumerate(entities):\n",
    "        if entity is not '0':\n",
    "            entity_locations_and_types.append((location, entity))\n",
    "    return entity_locations_and_types\n",
    "\n",
    "def get_entity_tags(utterance):\n",
    "    entity_locations_and_types = get_entity_types_and_locations(utterance)\n",
    "    split_utterance = utterance.split(' ')\n",
    "    tagged_entities = [(entity_type, split_utterance[location]) for location, entity_type in entity_locations_and_types]\n",
    "    return tagged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = 'set an alarm for five pm'\n",
    "get_entity_tags(utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's bring it all together, a full NLU engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Maybe give this function a better name?\n",
    "def get_NLU_results(utterance):\n",
    "    tagged_entities  = get_entity_tags(utterance)\n",
    "    return [utterance, predict_label(RF_model, utterance), tagged_entities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random test utterances I could come up with, maybe add some of your own and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [\n",
    "    'vacuum the bathroom',\n",
    "    'clean the hall',\n",
    "    'what is the weather like this weekend',\n",
    "     'what is the weather like in munich tomorrow',\n",
    "     'what is the temperature',\n",
    "     'will it rain today',\n",
    "     'turn off the kitchen lights',\n",
    "     'turn on the living room lights',\n",
    "     'set an alarm for five pm',\n",
    "     'set an alarm for ten am',\n",
    "     'what time is it in new york',\n",
    "     'what time is it in berlin in two hours from now',\n",
    "     'tell me a joke',\n",
    "     'how are you',\n",
    "     'when was biden born',\n",
    "     'how long does it take to boil an egg',\n",
    "     'how do you make a caesar salad',\n",
    "     'how much is a euro in dollars'\n",
    "]\n",
    "\n",
    "for utterance in utterances:\n",
    "    print(get_NLU_results(utterance))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a546bd87e52c3aef9807d84fb5760e9bbca867abb60a3fd598826e0271da7992"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
